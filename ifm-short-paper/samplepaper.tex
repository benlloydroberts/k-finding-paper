% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{cite}
\usepackage{booktabs}
\DeclareMathOperator*{\argmax}{argmax}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Reinforcement Learning  State Space Properties for Model Checking of Interlockings\thanks{Supported by Siemens Mobility UK \& EPSRC}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Ben Lloyd-Roberts\inst{1} \and
Phil James\inst{1} \and
Michael Edwards\inst{1}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.,
% If there are more than two authors, 'et al.' is used.
%


\institute{Swansea University, Swansea, UK  \\
\email{\{ben.lloyd-roberts, p.d.james, michael.edwards\}@swansea.ac.uk}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Railway interlockings serve as safety layer within railway signalling systems by ensuring proposed signalling requests are safe given the current state of the railway. As a vital part of any railway signalling system, interlockings are critical systems developed with the highest safety integrity level (SIL4) according to the CENELEC 50128 standard. The application of formal methods, in particular model-checking, in order to verify interlockings operate correctly is well established within academia and is beginning to see real applications in industry. However, the uptake of formal methods research within the UK rail industry has yet to make a substantial impact due to current approaches often producing false positives that require manual analysis during verification. Here, it is accepted that so-called invariants can be added to reduce the number of such false positives, however automatically computing these invariants remains a challenge. In this work we present a first approach illustrating how reinforcement learning can be used to learn properties of a state space for a given railway interlocking. Understanding how a verification problem can be formulated as one where machine learning can be successfully applied is the first step towards automated learning of invariants.

\keywords{Reinforcement Learning \and Interlocking \and Model Checking.}
\end{abstract}


\section{Introduction}

Interlockings serve as a filter or ‘safety layer’ between inputs from operators, such as route setting requests, ensuring proposed changes to the current railway state avoid safety conflicts. As a vital part of any railway signalling system, interlockings are critical systems regarded with the highest safety integrity level
(SIL4) according to the CENELEC 50128 standard. The application of model-checking
to Ladder Logic programs in order to verify interlockings is well established within academia and is beginning to see real applications in industry.
Bounded model-checking (BMC) is an efficient means of verifying a system through refutation. Given an abstracted model of a target system $M$ and some safety properties $\phi$, BMC searches for counterexamples up to some precomputed bound $k$. Search terminates when either an error trace is produced or the bound $k$ is reached. Determining this completeness threshold to sufficiently cover all states is often computationally intractable given it's true value will depend on model dynamics and size of the search space. Additionally, the k-induction rule, which checks if the property $\phi$ holds inductively for $k$ sequences of states, is constrained to acyclic graphs for guarantees of completeness. \\

An invariance rule allows us to establish an invariant property $\psi$ which holds for all initial states and transitions up to a given bound. Invariants may hold for sub-regions of the state space, meaning complete coverage isn't necessary to learn them. Supporting k-induction with strengthening invariants helps reduce the overall search space for bounded model checking, proving that invariant aspects of the system need not be considered. This can help filter cases where false negative counter examples are triggered by unreachable states. Generating sufficiently strong invariants is a non-trivial process given their construction is heavily program dependent. Usually such invariants require domain knowledge, typically devised by engineers responsible for the program implementation. \\

We infer two principle challenges to address. First, formulating theoretical and practical frameworks to represent invariant finding as a reinforcement learning task. Second, devising an appropriate probabilistic strategy for identifying learnable state space properties within those frameworks. With the ultimate aim of adapting our approach to identify ladder logic invariants, we present a set of preliminary results illustrating the potential for RL schemes to maximise state space coverage via learning progressively larger values for the recurrence reachability diameter.

\section{BMC \& Program Invariants}\label{sec:preliminaries}

\textbf{- Not sure if this section is needed?}


\section{Reinforcement Learning}
RL is a popular ML paradigm with demonstrably impressive capacity for modelling sequential decision making problems as the optimal control of some incompletely-known Markov Decision Process (MDP), known as the environment  \cite{sutton2018reinforcement}. Given a permitted set of actions performable over a series of discrete time steps $t$, software agent(s) interact with and observe changes in the environment, receiving intermittent reward signals. Over accelerated trial-and-error, a function or policy $\pi$, is learned mapping states to optimal actions likely to return the greatest cumulative future reward. Interactions with the environment can be characterised as continuous or episodic, depending on constraints of the learned task.
 
Tabular reinforcement learning has existed for decades through dynamic programming \cite{watkins1992q}, Monte-Carlo methods, and temporal difference learning. Resurgence in RL research over the last decade, applied to games \cite{schaul2015prioritized, silver2016mastering, vinyals2019grandmaster}, robotics \cite{gu2017deep, bloesch2022towards} and operations research \cite{mazyavkina2021reinforcement}, can be attributed to the fusion of a existing value/policy iteration methods with powerful approximate learning techniques following the advent of deep learning \cite{lecun2015deep}.   

Subsequent popularity in deep reinforcement learning (DRL) after breakthrough performance of deep Q-Learning on a set of gaming benchmarks \cite{mnih2013playing} gave rise to improvements of historic methods, now leveraging deep learning \cite{schulman2017trust}. Actor-Critic methods, combining policy learning and value function approximation saw particular successes.

Probabilistic learning is unlikely to provide guarantees of completeness, but can be used to supplement more suitable methods of doing so. For purposes of learning heuristics, we would like to maximise the information gathered, so that there is more data from which to identify patterns. With the aim of learning invariant properties that hold across states, state space coverage would be ideally maximised. Fortunately existing works have illustrated the efficacy of RL methods in learning such heuristics over large graph structures \cite{manchanda2019learning}. Distributing the task among concurrent workers for accelerated performance \cite{hoffman2020acme} and prioritising exploration in unfamiliar environments \cite{ostrovski2017countbased, haarnoja2018soft, gordillo2021improving}.

We select asynchronous advantage actor-critic (A3C) \cite{mnih2016asynchronous} to estimate both the value function and behaviour policy when exploring an environment. The asynchronous nature of the algorithm facilitates distributed exploration of state-action pairs via separate workers with a shared global policy network.


\section{Learning Framework}
Contacts, the binary variables constituting ladder logic program inputs, comprise the \emph{action space} $\mathcal{A}$. Transitions between program states are simulated via interactions with this action space, initially uniformly at random. Each state, formally the \emph{observation space} $\mathcal{S}$, is formed by the union of contacts and coils under their present valuation. Subsequently the environment unfolds as a set of reachable states for the respective ladder logic program. As workers improve their value estimates according to the reward function, a balance is maintained between stochastic action sampling and predictions from the policy network.

Reinforcing this behaviour requires motivating  agents via a sparse reward signal with potentially strong temporal dependence. In light of this, we influence agents to pursue the longest loop free path where a reward scheme positively rewards sequences of novel observations. Inversely, negative rewards are issued for repeated observations within a training episodes.
Workers are initialised with separate environment instances to accumulate experience independently. A global set of observations is asynchronously updated by each worker at set intervals to compile shared experiences.    

For practicality, each environment has an associated max number of episodes $T_{\max}$ to bound training runs. Additionally we utilise two forms of early termination to avoid superfluous interaction with the environment. First, if the worker performance curves converges to some local minima. Second, if all reachable states have been observed at least once. This update frequency depends on both the number of workers and precomputed size of the environment.

\section{Environment Generation}
Given exhaustive search of large state spaces is often computationally intractable, we generate a set of ladder logic programs where the number of reachable states and recurrence reachability diameter are known. Using existing models of ladder logic structures as a base template \cite{james2013verification}, we derive progressively larger programs by sequentially introducing additional rungs. This way a constrained yet predictable pattern of growth is devised. If $|S(p_i)|$ represents the number of reachable states for a program $p_i$, a subsequently generated program $p_{i+1}$ with one additional rung, has $2|S(p_i)|+1$ reachable states. Through a series of training runs on each environment we record the number of states observed by workers to gauge the overall state space coverage. 

\section{Results}
Preliminary results from applying our approach to a number of generated programs are outlined in Table \ref{tab:results}. Coverage metrics are expectedly maximised for environments with a small number of reachable states. Still, acceptable levels of coverage are observed for programs with more than $1\mathrm{e}{5}$ states. 

Disparate correlations between the coverage metrics and the total episodes may stem from the stochasticity of environment and network initialisation, leading to differing local minima. Could also have landed in desirable start state due to that random initialisation. 

We empirically observe three scenarios in the event environment coverage is not maximised: ($i$) Performance curves converge to a reward bound by the longest loop free path; ($ii$) Performance curves converge to a suboptimal reward; ($iii$) Performance improves linearly before collapsing to some suboptimal reward. 



The on-policy nature of actor critic meant previous experiences from old policy iterations were forgotten, thus biasing behaviour to the most recent model updates and introducing sample inefficiency.

Given the A3C algorithm requires workers to asynchronously update their shared network every $T_{\max}$ steps or on episode termination, larger values for $T_{\max}$ consolidate more information regarding worker trajectories before applying gradient updates to their local network. Therefore we observe a strong dependence on this parameter both in terms of increasing the k bound when introducing workers to larger environments. Prior to random episode initialisation and fine-tuning network update frequencies, workers seldom covered 100\% of the smaller environments.
\begin{table}[h]
	\caption{Initial results applying A3C learning over progressively larger environments. }
	\centering
	\arrayrulecolor[rgb]{0.906,0.91,0.914}
	\begin{tabular}{|c|c|c!{\color{black}\vrule}c|c|c|c|} 
		\arrayrulecolor{black}\hline
		\multicolumn{3}{!{\color{white}\vrule}c!{\color{black}\vrule}}{\textbf{Environment Metrics}}                                                                                                                                                                                                            & \multicolumn{4}{c!{\color{white}\vrule}}{\textbf{Training Metrics}}                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\ 
		\hline
		\multicolumn{1}{!{\color{white}\vrule}c!{\color{white}\vrule}}{{\cellcolor[rgb]{0.933,0.933,0.925}}\textbf{States}} & \multicolumn{1}{c!{\color{white}\vrule}}{\begin{tabular}[c]{@{}c@{}}\textbf{~Reachable~ }\\\textbf{ States }\end{tabular}} & {\cellcolor[rgb]{0.933,0.933,0.925}}\textbf{Actions} & \multicolumn{1}{c!{\color{white}\vrule}}{\textbf{Observed K~ }} & \multicolumn{1}{c!{\color{white}\vrule}}{{\cellcolor[rgb]{0.933,0.933,0.925}}\begin{tabular}[c]{@{}>{\cellcolor[rgb]{0.933,0.933,0.925}}c@{}}\textbf{States}\\\textbf{Observed~~ }\end{tabular}} & \multicolumn{1}{c!{\color{white}\vrule}}{\textbf{Coverage}} & \multicolumn{1}{c!{\color{white}\vrule}}{{\cellcolor[rgb]{0.933,0.933,0.925}}\begin{tabular}[c]{@{}>{\cellcolor[rgb]{0.933,0.933,0.925}}c@{}}\textbf{Total }\\\textbf{~ Episodes }\end{tabular}}  \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{12}                                                           & 7                                                                                                                          & {\cellcolor[rgb]{0.933,0.933,0.925}}2                & 6                                                          & {\cellcolor[rgb]{0.933,0.933,0.925}}7                                                                                                                                                            & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+04                                                                                                                                                      \\ 
		\arrayrulecolor[rgb]{0.906,0.91,0.914}\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{14}                                                           & 15                                                                                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}4                & 14                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}15                                                                                                                                                           & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+04                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{16}                                                           & 31                                                                                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}8                & 28                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}31                                                                                                                                                           & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+04                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{18}                                                           & 63                                                                                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}16               & 48                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}63                                                                                                                                                           & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{20}                                                           & 127                                                                                                                        & {\cellcolor[rgb]{0.933,0.933,0.925}}32               & 33                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}127                                                                                                                                                          & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{22}                                                           & 255                                                                                                                        & {\cellcolor[rgb]{0.933,0.933,0.925}}64               & 76                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}255                                                                                                                                                          & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+04                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{24}                                                           & 511                                                                                                                        & {\cellcolor[rgb]{0.933,0.933,0.925}}128              & 49                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}377                                                                                                                                                          & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}1.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{26}                                                           & 1023                                                                                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}256              & 306                                                        & {\cellcolor[rgb]{0.933,0.933,0.925}}1023                                                                                                                                                         & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{28}                                                           & 2047                                                                                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}512              & 538                                                        & {\cellcolor[rgb]{0.933,0.933,0.925}}2047                                                                                                                                                         & 100                                                         & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{30}                                                           & 4095                                                                                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}1024             & 1418                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}4084                                                                                                                                                         & 99.731                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{32}                                                           & 8191                                                                                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}2048             & 1712                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}7907                                                                                                                                                         & 96.532                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{34}                                                           & 16383                                                                                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}4096             & 1498                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}15654                                                                                                                                                        & 95.550                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{36}                                                           & 32767                                                                                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}8192             & 2879                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}27752                                                                                                                                                        & 84.694                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{38}                                                           & 65535                                                                                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}16384            & 1969                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}58373                                                                                                                                                        & 89.071                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{40}                                                           & 131071                                                                                                                     & {\cellcolor[rgb]{0.933,0.933,0.925}}32768            & 2692                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}108638                                                                                                                                                       & 82.884                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}2.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{42}                                                           & 262143                                                                                                                     & {\cellcolor[rgb]{0.933,0.933,0.925}}65536            & 1406                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}5199317                                                                                                                                                      & 76.033                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{44}                                                           & 524287                                                                                                                     & {\cellcolor[rgb]{0.933,0.933,0.925}}131072           & 1782                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}325781                                                                                                                                                       & 62.137                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}2.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{46}                                                           & 1048575                                                                                                                    & {\cellcolor[rgb]{0.933,0.933,0.925}}262144           & 1593                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}671645                                                                                                                                                       & 64.053                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{48}                                                           & 2097151                                                                                                                    & {\cellcolor[rgb]{0.933,0.933,0.925}}524288           & 1598                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}1206867                                                                                                                                                      & 57.547                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}3.00E+05                                                                                                                                                      \\ 
		\hline
		{\cellcolor[rgb]{0.933,0.933,0.925}}2\textsuperscript{50}                                                           & 4194303                                                                                                                    & {\cellcolor[rgb]{0.933,0.933,0.925}}1048576          & 2527                                                       & {\cellcolor[rgb]{0.933,0.933,0.925}}1739939                                                                                                                                                      & 41.48                                                      & {\cellcolor[rgb]{0.933,0.933,0.925}}1.50E+05                                                                                                                                                      \\
		\hline
	\end{tabular}
	\label{tab:results}
	\arrayrulecolor{black}
\end{table}



\section{Conclusion \& Future Work }
A3C shows some promising preliminary results but is clearly limited in its capacity to scale. 

The algorithm shows proclivity to collapse following small network adjustments in previously unexplored regions of large environments. 

Introduce experience replay for distributed learning to improve on-policy bias and sample efficiency. Additional algorithms to implement for scalability - we can guarantee the increase of action spaces when transitioning to industry scale ladder logic programs where the number of contacts are known. It may be worth exploring DDPG to improve learning solely based on the observation space. 

Intrinsic motivation for exploration.

IMPALA to improve both sample efficiency over A3C and robustness to network architectures and hyperparameters. Use of LSTM also improves performance given GPU acceleration is maximised on larger batch updates. A3C in our setting experiences variable episodic updates.


 
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
\bibliographystyle{splncs04}
\bibliography{bibliography}
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%

\end{document}
