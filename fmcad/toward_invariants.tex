
%% bare_conf_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference,compsoc]{IEEEtran}
% Some/most Computer Society conferences require the compsoc mode option,
% but others may want the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.
\usepackage[colorinlistoftodos]{todonotes}
\setuptodonotes{inline}






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphicssetuptodonotes
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath, amssymb}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Towards Reinforcement Learning of Invariants for Model Checking of Interlockings}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Ben Lloyd-Roberts, Phillip James, Michael Edwards}
\IEEEauthorblockA{Computational Foundry, Swansea University\\
Swansea, United Kingdom\\
Email: \{ben.lloyd-roberts, p.d.james, michael.edwards\}@swansea.ac.uk}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page (and note that there is less available width in this regard for
% compsoc conferences compared to traditional conferences), use this
% alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\listoftodos


\maketitle


% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
The application of formal methods, in particular model checking, to verify interlockings operate correctly is well established within academia and is beginning to see real applications in industry. However, the uptake of formal methods research within the UK rail industry has yet to make a substantial impact due to current approaches often producing false positives that require manual analysis during verification. Here, it is accepted that so-called invariants, properties which hold for the entirety or a substantial subregion of the search space, can help reduce the number of such false positives. Invariants are often bespoke, manually designed by engineers making their automatic generation a challenge. In this work we present first steps towards using reinforcement learning to navigate state space representations of ladder logic programs and generate a dataset of state sequences from which invariants could be mined. 
\todo{Results and what they suggest}
\end{abstract}

% no keywords


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



% no \IEEEPARstart
\section{Introduction}


% Builing on existing approaches to model ladder logic dynamics
% Use of LLP semantics to extract its inputs / outputs to model state transitions 
% Provides an interactive environment for software agents (vars comprise the state and inputs the means to move between them)
% RL provides a sensible inerface for learning patterns over finite graph structures where software agents have autonomy in how they choose to interact with the model, based on a rather simplistic reward scheme and reset logic
% Probabilistic methods won't provide guarantees of completeness but may be able to provide useful heuristics for more comprehensive verification approaches
% Our aim is to incentivise agents to explore long sequences of novel states for state coverage maximisation.
% Motivation for this is two-fold:
% (1) Invariants should hold for substanital subregions or entirety of LLP's reachable state space, therfore agents should try to observe as many unique states / transitions as possible. (they may also hold for unreachable states but we're not interested in them)
% (2) Agent behaviour molded by reward scheme (and some other optional variables) and this must reflect our coverage objective - agents must be rewarded for discovering new states and penalised for state loops (they've potential to get stuck in subregions otherwise and will soley rely on any explorative parameters during training).
% Exploration phase then automatically generates a dataset of state sequences (known as trajectories) from which we hope to mine invariant properties.

In this work we build upon existing techniques in modelling LLP dynamics. First we propose a theoretical and practical framework for navigating state space representations of LLPs as a goal-orientated reinforcement learning task. Paired with selective reset logic during the training phase, see section \ref{subsect:MDP}, software agents are incentivised to maximise state space coverage by pursuing sequences of unobserved states, i.e the max acyclic subgraph, for a given interlocking program.   During the agent's exploration phase state sequences are generated, from which we eventually hope to mine invariant properties. 

 \todo{Elaborate on invariant finding problem}



\section{Preliminaries}\label{sec:preliminaries}
We now briefly discuss  model checking of railway interlockings and reinforcement learning. For further details we refer the reader to \cite{kanso2009automated, james2013verification} and \cite{mnih2016asynchronous} respectively.

\subsection{Ladder Logic \& Interlockings}
Interlockings serve as a filter or ‘safety layer’ between inputs from railway operators, such as route setting requests, ensuring proposed changes to the current railway state avoid safety conflicts. As a vital part of any railway signalling system, interlockings are critical systems regarded with the highest safety integrity level
(SIL4) according to the CENELEC 50128 standard. 

Ladder logic is a graphical language widely used to program Programmable Lofic Controllers\cite{IECstandard} and in partiuclar the Siemens interlocking systems we consider in this work. Ladder logic gets its name from its graphical ``ladder''-like form of the graphical programs that are written.  From an abstract perspective, ladder logic diagrams can be seen to represent
propositional formulae, here we follow the definition of James et al|\cite{sefm14}. A ladder logic rung consists of the following entities.
%(see~\fig{ladderconnectives}).
\emph{Coils}
%are used to
represent boolean values that are stored for later use as output
variables from the program. A coil is always the right most entity
of the rung and its value is computed by executing the rung from left
to right.
\emph{Contacts} are the boolean inputs of a rung,
with \emph{open} and \emph{closed} contacts representing the values
of un-negated and negated variables respectively.  The value of a coil
is calculated when a rung fires, making use of the current set of
inputs -- input variables, previous output variables,
and output variables already computed for this cycle --
following the given connections.  A horizontal connection between
contacts represents logical conjunction and a vertical connection
represents logical disjunction.

A interlocking executes such a program froim top-to-bottom over and over, indefinitely.

More formally, a ladder logic program is constructed in terms of disjoint finite sets
$I$ and $C$ of input and output variables. In our example in
Fig.~\ref{fig:pelicanladder}, we have $I=\{pressed\}$ and $C =
\{crossing, req, tlag, tlbg, tlar, tlbr, plag, plbg, plar, plbr\}$.
We define $C' = \{ c' \, | \, c \in C \}$ to be a set of new variables
(intended to denote the output variables computed in the current
cycle).

\textbf{Definition:Ladder Logic Formulae}\todo{make definition}
A ladder logic formula $\psi$ is a propositional formula 
%composed from a set of input variables $I$ and a
%set of output variables $C$ that satisfies the following conditions:
of the form
$$\psi \equiv ((c'_1 \leftrightarrow \psi_1) \wedge (c'_2 \leftrightarrow
\psi_2) \wedge  \ldots \wedge (c'_n \leftrightarrow \psi_n)$$ 
%
%For some $n \geq 0$ where each $\psi_i$  with $1 \leq i \leq n$ is a
%propositional formula which has the following form.
such that the following holds for all $i,j\in \{1,\ldots,n\}$:
\begin{itemize}
\item $c'_i \in C'$
\item $i \neq j \rightarrow c'_i \neq c'_j$
\item $vars(\psi_i) \subseteq I \cup  \{c'_1, \ldots, c'_{i-1} \} \cup \{ c_i, \ldots , c_n \}$
\end{itemize}


\textbf{Remark:} Note that the output variable $c_i'$
of  each rung $\psi_i$, may depend on  $\{ c_i, \ldots , c_n \}$ from the previous cycle, 
but not on $c_j$ with $j<i$, due to the imperative nature of the ladder logic implementation. 
Those values are overridden.


% Def 1 from 

\subsection{Verification of Interlockings}

Model checking is a formal verification technique stemming from the need to systematically
check whether certain properties hold for different configurations (states) of a given system.
Given a finite transition system $T$ and a formula $F$, model checking attempts to verify through refutation that $s \vdash F$ for every system state $s \in T$, such that $T \vdash F$. 

The application of model checking
to Ladder Logic programs (LLPs) in order to verify interlockings is well established within
academia and is beginning to see real applications in industry.  As early as 1995, Groote
et al. ~\cite{groote1995safety} applied formal methods to verify an interlocking for controlling the Hoorn-Kersenbooger railway station. They conjecture the feasibility of verification techniques
as means of ensuring correctness criteria on larger railway yards. In 1998, Fokkink
and Hollingshead ~\cite{fokkink1998verification} suggested a systematic translation of Ladder Logic into Boolean
formulae. Newer approaches to interlocking verification have also been proposed in recent
years ~\cite{fantechi2012some, ferrari2011model, haxthausen2008modelling}. This includes work by Linh et al. which explores the verification
of interlockings written in a similar language to Ladder Logic using SAT-based model
checking. After two decades of research, academic work ~\cite{kanso2009automated, james2013verification} has shown that verification
approaches for Ladder Logic can indeed scale; in an industrial pilot, Duggan et al. [14] conclude: “Formal proof as a means to verify safety has matured to the point where it
can be applied for any railway interlocking system.” In spite of this, such approaches still lack widespread use within the Rail industry. 


In particular, one of the limitations of such model checking solutions is that verification can fail due to over approximation of the model being checked, typically when using techniques such as inductive verification~\cite{}. Such inductive verification checks to see if a given state satisfies some condition but does not consider whether these states which violate
the same safety condition are reachable by the system. These false positive  counter examples often require manual inspection by an
experienced engineer. Here, one solution that is proposed~\cite{1688959} is to introduce so-called invariants to suppress false positives. Invariants are properties that hold for sub-regions of the state space. The aim is to introduce invariants that help bound the region of reachable states when model checking. However generating sufficiently strong invariants automatically is a complex task, one which has received considerable attention in academic
literature. From software engineering techniques \cite{case2007automated, bensalem1996powerful} to hybrid methods incorporating machine learning \cite{garg2016learning}, researchers have proposed various approaches to invariant finding with varying degrees of success. and in this work we take the first steps towards using machine learning to generate invariant by provided a formnal mapping to machine learning and an analysis of state exploration by various algorithms based around reinforcement learning.


\subsubsection{Transistion Systems and Model Checking for Ladded Logic}
For this work, we have concentrated on trying to produce invariants for the approaches taken by Kanso et al.~\cite{kanso2009automated} and James et al.~\cite{james2013verification}. Here we include their model of ladder logic based railway interlocking programs as we use this as a basis for defining a learning environment. \todo{Explain why the LTS in James et al. [17] uses four elements (transition label) but our version doesn't}
\subsection{Modelling Ladder Logic}
Builiding upon the propositional representation of a ladder logic program given in Section~\ref{}.

\textbf{Semantics of Ladder Logic Formulae}

% Let $\{0,1\}$ represent the set of boolean values and let
% \begin{align*}
% \Val_I  &= \{ \mu_I \, | \, \mu_I : I \to \{0 , 1 \} \}  = \{0,1 \}^I   \\
% \Val_C &=  \{ \mu_C \, | \, \mu_C : C \to \{ 0 , 1 \} \} = \{ 0,1 \}^C 
% \end{align*}
% be the sets of valuations for input and output variables.
% The semantics of a ladder logic formula $\psi$ is 
% a function that takes the two current valuations and returns a new
% valuation for output variables.
% %
% \begin{align*}
% & [ \psi ] : \Val_I \times \Val_C \to  \Val_C \\
% & [ \psi ] ( \mu_I, \mu_C ) = \mu'_C 
% \end{align*}
% %where $\mu'_C$ is computed as follows: the value of each variable $c_i$ is computed
% %using the $i$th rung of the ladder, $\psi_i$, using the valuations $\mu_C$ and $mu_I$ %from the last cycle and the current valuations restricted to those evaluated be fore the %current variable. The variable remains constant if it does not corresponding ladder logic %rung.
% where
% \begin{align*}
% &\mu'_C (c_i) = [ \psi_i] ( \mu_I, (\mu_C)_{\upharpoonright
% \{c_i, \ldots , c_n \}} , (\mu'_C\circ \unprime)_{\upharpoonright
% \{c'_1, \ldots , c'_{i-1} \}}) \\
% & \mu'_C(c) = \mu_C(c) \  \textbf{if} \ c \notin \{ c_1, \ldots , c_n \} 
% \end{align*}
% and $[ \psi_i](\cdot,\cdot,\cdot)$ denotes the usual value of a propositional formula under a valuation.


% Next we make use of the above to form a labelled transition system
% representing the ladder logic program.

% \textbf{Ladder Logic Labelled transition System}
% %, reachability]
% We define the labelled transition system $\rm LTS(\psi)$ for a ladder logic formula $\psi$ to be the four tuple $(\Val_C,\Val_I,\rightarrow, \Val_0)$ 
% %
% \vspace{-1ex}
% \begin{itemize}
% \item $\Val_C$ is a finite set of states.
% \vspace{-1ex}
% \item $\Val_I$ is a finite set of transition labels.
% \vspace{-1ex}
% \item $\rightarrow \subseteq S \times T \times S $ is a labelled transition relation.
% \vspace{-1ex}
% \item $\Val_0 \subseteq S$ is the set of initial states.
% \vspace{-1ex}
% \end{itemize}
% %
% We write $ s \xrightarrow{t} s'$ for $(s,t,s')\in R$.
% A state $s$ is called \emph{reachable} if 
% $s_0 \xrightarrow{t_0} s_{1} \xrightarrow{t_1} \ldots 
% \xrightarrow{t_{n-1}} s_n$,  
% for some states $s_0,\ldots, s_{n} \in S$, and 
% labels $t_0, \ldots, t_{n-1} \in T$ such that
% $s_0 \in S_0$ and $s_n = s$.  
% \end{mydef}

% \begin{mydef}[Ladder Logic Labelled Transition System]
% We define the labelled transition system $\rm LTS(\psi)$ for a ladder logic formula $\psi$ to be the four tuple $(\Val_C,\Val_I,\rightarrow, \Val_0)$ 
% %of the  finite set of states $\Val_C$,  a finite set of labels $\Val_I$, a transition relation $\rightarrow \s%ubseteq S \times T \times S$ and a set of initial states $S_0 \subseteq S$ as follows:
% where
% \begin{itemize}
% %\item $\Val_C = \{ \mu_C | \mu_C : C \to \{ 0 , 1 \} \}   $
% %\item $\Val_I = \{ \mu_I | \mu_I : I \to \{0 , 1 \} \} $
% \item $\mu_C \xrightarrow{\mu_I} \mu'_C$ iff  $[ \psi ] ( \mu_I ,
%   \mu_C) = \mu'_C$
% \item $\Val_0 = \{\mu_C \, | \, \mu_C \mbox{ inital valuation}\}$
% \end{itemize}
% \end{mydef}








\subsection{Reinforcement Learning}
Reinforcement Learning (RL) is a popular machine learning paradigm with demonstrably impressive capacity for modelling sequential decision making problems as the optimal control of some incompletely-known Markov Decision Process (MDP)~\cite{sutton2018reinforcement}. This user defined \textit{environment}, $\mathcal{E}$, comprises a set of unique states $\mathcal{S}$, a set of permitted actions from each state $\mathcal{A}(s)$, a function describing state transitions $f : S \times \mathcal{A} \to S^{+}$ and a scalar reward signal $r_t$. States, actions and rewards are measured over discrete time steps $t$, where simulations are summarised through the \textit{trajectory}, $\tau = (s_1, a_1, r_1,  s_2, a_2, r_2,...,s_h, a_h, r_h)$, where $h$ refers to the \textit{horizon}, a time step beyond which rewards are no longer considered. Through repeated simulations, software agents aim to optimise a deterministic or stochastic behaviour function known as the policy, denoted $\pi(s)$ or  $\pi(a|s)$ respectively, mapping MDP states to optimal actions expected to maximise cumulative rewards over time. We refer to the summary reward objective as the \textit{expected return}, $\mathbb{E}=[G_t | S_t = s, A_t = a]$, an empirical average taken over future \textit{discounted returns} from the current time step $G_{t} = \sum_{i=t}^{T}\gamma^{i-t}r_{i}$. Here, $T$ refers to a terminal time step where simulation ends according to some stopping criteria. Thereafter, learning may conclude having achieved adequate performance or resume from some initial state following an environment reset. The discount factor $\gamma \in [0,1]$ applies to successive rewards at each time step to help enumerate returns over a potentially infinite horizon. 

Resurgence in RL research over the last decade has seen applications in games~\cite{schaul2015prioritized, silver2016mastering, vinyals2019grandmaster}, robotics~\cite{gu2017deep, bloesch2022towards} and operations research~\cite{mazyavkina2021reinforcement} and can be attributed to the fusion of existing methods~\cite{watkins1992q} with powerful approximate learning techniques~\cite{lecun2015deep} following the advent of deep learning~\cite{mnih2013playing}. Subsequent popularity in deep reinforcement learning (DRL) gave rise to improvements of historic methods~\cite{schulman2017trust}. Actor-Critic methods, combining policy learning~\cite{kakade2001natural} and value function approximation saw particular successes in establishing state-of-the-art performance~\cite{schulman2017proximal}.

Probabilistic learning is unlikely to provide guarantees of completeness, but can be used to supplement formal methods, such as model checking, via learned heuristics. We posit an approach of information maximisation, collecting sufficient trajectories from which to identify patterns or sequences. With the aim of learning invariant properties that hold across states, state space coverage should be maximised. 

Throughout this work we have used asynchronous advantage actor-critic (A3C) to estimate both the value function and behaviour policy while exploring an environment. The asynchronous nature of the algorithm facilitates distributed exploration of state-action pairs via separate workers with a shared global policy network.


\section{Mapping Formal Methods to Reinforcement Learning} \label{sec:mapping_fm_to_ml}
....

\subsection{Ladder Logic Markov Decision Process}\label{subsect:MDP}
We now define the finite Markov Decision Process (MDP), or environment $\mathcal{E}$ used to represent the LLP. A Ladder Logic MDP $M(\psi)$ is a five tuple $\langle S,\mathcal{A},P_a(s,s^\prime), R_a(s,s^\prime),\gamma \rangle$, where 
\begin{itemize}
	\item $S = V_I \cup V_C$, observation space to represent the MDP state at discrete time steps.
	\item $\mathcal{A} = V_I$, describes the action space; a set of formally defined actions which change the observation space
	\item $P_a(s,s^\prime) = Pr(s_{t+1} = s^\prime | s_t, a_t)$, describing the likelihood of observing state $s_{t+1}$ given action $a_t$ taken from state $s_t$
	\item $R_a(s,s^\prime)$ is a reward function fed back to the agent at each time step
	\item $\gamma$ is a discount scalar applied to the reward estimates for future time steps.
\end{itemize}

Subsequently the environment unfolds as a set of reachable states for the respective LLP. As workers improve their value estimates according to the reward function, a balance is maintained between stochastic action sampling for exploration and best predictions from the policy network.

Given invariant properties hold for some subregion of program states, we aim to reinforce exploration to maximise MDP coverage. In light of this, we influence agents to pursue the longest loop free path, or max $k$ value for Bounded Model Checking. Consequently we design a reward scheme which positively rewards sequences of novel observations. Inversely, negative rewards are issued for repeated observations within the same training episodes. Workers are initialised with separate environment instances to accumulate experience independently. A global set of observations is asynchronously updated by workers periodically to compile shared experiences.    

For practicality, each environment has an associated max number of episodes $T_{\max}$ to constrain runtime. We utilise two forms of early termination to avoid superfluous training. First, if worker performance curves converges to some local minima, i.e consecutive model updates result in no further improvement. Second, training ends in our artificially generated programs if all reachable states have been observed at least once. To aid exploration, on episode resets we randomly initialise workers in some previously visited state, as demonstrated in~\cite{gordillo2021improving}. 

\section{Related Work}
Here we briefly highlight key contributions within related literature, addressing the invariant finding problem for interlocking programs and contemporary RL strategies for environment exploration.

\subsection{Invariant Finding}


\subsection{Reinforcement Learning}
Existing works have illustrated the efficacy of RL methods in learning such heuristics over large graph structures~\cite{manchanda2019learning}, distributing learning for accelerated performance~\cite{hoffman2020acme} and prioritising novelty when exploration unfamiliar environments~\cite{ostrovski2017countbased, haarnoja2018soft, gordillo2021improving}.

\section{Results}
We now present a set of results from applying our approach to a series of generated LLPs, modelled as learning environments. 
\subsection{Environment Generation}

Given exhaustive search of large state spaces is often computationally intractable, we have generated a set of LLPs where the number of reachable states and recurrence reachability diameter are known. This has enabled us to analyse the performance of our approach against well understood state spaces. Using existing models of ladder logic structures as a base template~\cite{james2013verification}, we derive progressively larger programs by sequentially introducing additional rungs. This way a constrained yet predictable pattern of growth is devised. If $|S(p_i)|$ represents the number of reachable states for a program $p_i$, a subsequently generated program $p_{i+1}$ with one additional rung, has $2|S(p_i)|+1$ reachable states. Through a series of training runs on each environment we record the number of states observed by workers to gauge the overall state space coverage. 


\subsection{Discussion}
\todo{Elaborate on the efficacy of approach - is it actually promising?}
\todo{Add results on some Loch Ness interlocking examples}
\todo{Compare existing BCM approaches to learning heuristic?}
Preliminary results applying our approach to a number of generated programs are outlined in Table~\ref{tab:results}. `Actions' referenced in column 3 refer to the number of possible assignments over input variables in each LLP. `$K$', refers to the greatest number of steps taken before repeating observations n the environment, across all workers.

Coverage metrics are expectedly maximised for environments with a small number of reachable states with acceptable levels of coverage for programs with more than $1\mathrm{e}{5}$ states. Interestingly, we observed longer training durations occasionally increased coverage beyond a certain threshold. It is possible workers learn an optimal search strategy within a subregion of the state space. Additionally, performance in terms of max $k$ and states reached increased by approx. $5\%$ when decreasing the total number of episodes from 3e5 to 1.5e5 episodes. \todo{Fix scientific notation} This may be a product of random episode initialisation spawning workers in more desirable states where stochastic action sampling happened to lead to unfamiliar subregions of the environment.

Performance plots illustrating the cumulative reward  which failed to maximise coverage often increased linearly before collapsing to some suboptimal reward. \todo{Include performance plots} This may be due to tendencies for large network updates to shift the network gradients into a bad local minima, from which performance does not recover within the allotted training duration. The on-policy nature of actor critic means trajectories generated via an old policy are no longer sampled during minibatch updates for the current policy,  thus biasing behaviour to the most recent model updates and introducing sample inefficiency. Adding experience replay \cite{wang2017sample} may help avoid this in future applications

Given the A3C algorithm requires workers to asynchronously update their shared network every $T_{\max}$ steps or on episode termination, larger values for $T_{\max}$ consolidate more information regarding worker trajectories before applying gradient updates to their local network. We found the most significant improvements to performance in terms of coverage metrics and increasing the $k$ bound when introducing workers to larger environments, was lower update frequencies and random start state initialisation. Prior to these adjustments workers, irrespective of their number, seldom covered 80\% of most smaller environments. Similarly, for the largest environment with $2^{50}$ states, coverage improved from 3.2\% to 41.48\%



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\section{Conclusion \& Future Work }
In this paper we have applied a basic asynchronous deep reinforcement learning method to maximise program state coverage, motivated by a reward scheme \todo{Sentence?}. some promising preliminary results but limited in its capacity to scale across large observation spaces. 

In light of our findings, we aim to improve several aspects of our approach, predominantly concerning learning stability, sample efficiency and training speed. Experience replay for distributed learning may improve on-policy bias and sample efficiency. 

The low dimensionality of our state spaces representation may allow us to introduce count-based exploration models to dampen the reward issued for states repeatedly observed~\cite{ostrovski2017countbased}. Intrinsic motivation has also illustrated successes in environment exploration~\cite{houthooft2017vime}.

Applying IMPALA~\cite{espeholt2018impala} to improve both sample efficiency over A3C and robustness to network architectures and hyperparameter adjustments. The adoption of a Long Short-Term Memory model (LSTM) also improves performance given GPU acceleration is maximised on larger batch updates.




% conference papers do not normally have an appendix



% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi
We thank Tom Werner and Andrew Lawrence at Siemens Mobility UK \& EPSRC for their support in these works.
% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{bibliography}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)




% that's all folks
\end{document}


